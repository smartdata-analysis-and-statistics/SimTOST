---
title: "Sample Size Calculation for Trials with 2 arms and 3 endpoints"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Sample Size Calculation for Trials with 2 arms and 3 endpoints}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteEncoding{UTF-8}
bibliography: '`r system.file("references.bib", package="simsamplesize")`'
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
require(kableExtra)
```


It is often required to investigate equivalence for more than one primary variable. For example, the European Medicines Agency (EMA) recommends showing equivalence both for AUC and Cmax.

A decision must be made as to whether it is desirable to 

- Demonstrate equivalence for **all** primary endpoints: most common setting (also known as *multiple co-primary endpoints*)
- Demonstrate equivalence for **at least one** of the primary endpoints (also known as *multiple primary endpoints*)



# Testing the Difference of Means (DOM) for multiple Co-primary Endpoints
In biosimilar development, it is desirable that equivalence is shown in all doses, routes of administration, patient populations, and endpoints.
To establish individual equivalence between the two treatments, $\mu_{T}^{(j)} - \mu_{R}^{(j)}$, needs to lie within a reasonable range around zero for **all** primary endpoints. The null hypotheses of the individual equivalence test are expressed as follows:

$$H_0: \mu_T^{(j)} - \mu_R^{(j)} \le E_L ~~ \text{or}~~ \mu_T^{(j)} - \mu_R^{(j)} \ge E_U \quad \text{for at least one}\;j$$

Alternative hypothesis: the two treatments are equivalent

$$H_1: E_L<\mu_{T}^{(j)}-\mu_{R}^{(j)} < E_U \quad\text{for all}\;j$$

The null hypothesis $H_0$ is rejected if and only if all of the null hypotheses associated with each of the $K$ primary endpoints are rejected at a significance level of $\alpha$.


Because all hypotheses have to be rejected, there is no multiplicity issue and the Type I error rate is controlled. However, there is a decrease in power because the type II error increases together with the number of endpoints to be evaluated [@mielke_sample_2018].


## Example data
As an illustrative example, we consider published data from the phase-1 trial [NCT01922336](https://clinicaltrials.gov/study/NCT01922336#study-overview). In this trial, the following outcomes were observed after a single dose of SB2 or its EU-sourced reference product [@shin_randomized_2015]:

```{r, echo=FALSE}
data <- data.frame("PK measure" = c("AUCinf ($\\mu$g*h/mL)","AUClast ($\\mu$g*h/mL)","Cmax ($\\mu$g/mL)"),
                   "SB2" = c("38,703 $\\pm$ 11,114", "36,862 $\\pm$ 9133", "127.0 $\\pm$ 16.9"), 
                   "EU-INF" = c("39,360  $\\pm$ 12,332", "37,022 $\\pm$ 9398", "126.2 $\\pm$ 17.9"))

kableExtra::kable_styling(kableExtra::kable(data, 
                                            col.names = c("PK measure", "SB2", "EU-INF"),
                                            caption = "Primary PK measures between test and reference product. Data represent arithmetic mean +- standard deviation."),
                          bootstrap_options = "striped")

```

We adopt the $80\%/125\%$ rule to define the equivalence bounds in terms of the reference mean value, and use a one-sided significance level of 0.05 to evaluate the average bioequivalence. The target power is 90\%.

If we were to test each PK measure independently, we would find a total sample size of $N=76$ for AUCinf, $N=56$ for AUClast, and $N=20$ for Cmax. This means that we would have to enroll $76+56+20 = 152$ patients in order to reject $H_0$ at a significance level of 5\%.





To provide the necessary information for the calculations, the user can input the mean (`mu_list`) and standard deviation information for each endpoint on each arm using lists of standard deviations (`sigma_list`). Since in this example, we only have the standard deviation of the endpoints, the user needs to provide correlation information between endpoints of each arm. This can be achieved through a correlation matrix (`cor_mat`) or by specifying a constant correlation parameter between endpoints (`rho`), which will be internally used to calculate a variance-covariance matrix across endpoints on each arm, if none of them are provided by default it is assumed no correlation between endpoints on all the arms (`rho=0`). However, if the user possesses the variance-covariance matrix per arm, they can include it as a list of matrices in the `varcov_list` parameter.

The function will automatically validate the information provided for each arm and exclude endpoints with missing values (NA) for either the mean or variance. Additionally, it is recommended that the user supplies a list of endpoint names (`ynames_list`) to help the program correctly assign the mean and variance information for each endpoint in each arm. As we only consider three endpoints in this example, we will specify `ynames_list` as the list of `AUCinf`, `AUClast`, and `Cmax`. 

The user must also provide information about the comparators using two lists: `list_comparators` and `list_y_comparator`. The `list_comparators` identifies the arms to be compared within each comparator, while the `list_y_comparator` specifies the endpoints to be compared within each comparator. If no `list_comparators` are provided, the function by default will use all possible pairwise comparisons across arms. Similarly, when `list_y_comparator` is not specified, it will compare all the endpoints common in both arms.

The function will verify if there is enough information available to perform the comparison, considering both mean and variance for each endpoint in each arm. If complete information is not provided, the function will perform comparisons only for endpoints with full information on each arm. In this case, as we only compare two arms, we will only compare SB2 and EU Remicade.


## No Correlation between Endpoints
In this setting, we assume a common standard deviation for each biosimilar, Ratio of Means (ROM) as parameter tested, parallel design, lognormal distribution assumption, no multiplicity adjustment, and equivalence is only needed for one of the endpoints. Besides, we assume that all endpoints are uncorrelated. This is specified with the default value of the rho parameter, which is set to $\rho = 0$.

```{r}
mu_list <- list(SB2 = c(AUCinf = 38703, AUClast = 36862, Cmax = 127.0),
                EUREF = c(AUCinf = 39360, AUClast = 37022, Cmax = 126.2))

sigma_list <- list(SB2 = c(AUCinf = 11114, AUClast = 9133, Cmax = 16.9),
                   EUREF = c(AUCinf = 12332, AUClast = 9398, Cmax = 17.9))
```

As we are comparing multiple co-primary endpoints, it's necessary to specify the lower and upper equivalence boundaries for each endpoint. For simplicity, we assume the same equivalent boundaries for all endpoints. 

```{r}
# Equivalent boundaries
lequi.tol <- c(AUCinf = 0.8, AUClast = 0.8, Cmax = 0.8)
uequi.tol <- c(AUCinf = 1.25, AUClast = 1.25, Cmax = 1.25)
```

By default, it is required that all $k=m$ co-primary endpoints have to be equivalent:

```{r}
library(simsamplesize)

N_ss <- calopt(power = 0.9, # target power
               alpha = 0.05,
               mu_list = mu_list,
               sigma_list = sigma_list,
               lequi.tol = lequi.tol,
               uequi.tol = uequi.tol,
               dtype = "parallel",
               ctype = "ROM",
               vareq = TRUE,
               lognorm = TRUE,
               ncores = 1,
               nsim = 50,
               seed = 1234)
kable(N_ss$response)
```

If we increase `nsim` to 10,000 we find a total sample size of 80 patients.



Using `plot.simms()` function, we can make the plot of sample size vs power achieved, as follows:

```{r }
plot.simss(N_ss)
```




# Testing the Difference of Means (DOM) for multiple Primary Endpoints
In a clinical trial with a single endpoint tested at $\alpha = 0.05$, the probability of rejecting $H_0$ by chance alone is at most 0.05.  By contrast, if there are two independent endpoints, each tested at $\alpha = 0.05$, and if 260 success on either endpoint by itself would lead to a conclusion of a drug effect, there is a 261 multiplicity problem.  For each endpoint individually, there is at most a 5 percent chance of 262 finding a treatment effect when there is no effect on the endpoint, and the chance of erroneously 263 finding a treatment effect on at least one of the endpoints (a false positive finding) is about 10 264 percent.  More precisely, when the endpoints are independent, there is a 95 percent chance of 265 correctly failing to detect an effect for each endpoint if there is no true effect for either endpoint.  266 The chance of correctly failing to detect an effect on both endpoints together is thus 0.95 * 0.95, 267 which equals 0.9025, and so the probability of falsely detecting an effect on at least one endpoint 268 is 1 - 0.9025, which equals 0.0975.  Without correction, the chance of making a Type I error for 269 the study as a whole would be 0.1 and the study-wise Type I error rate is therefore not 270 adequately controlled.  The problem is exacerbated when more than two endpoints are 271 considered.  For three endpoints, the Type I error rate is 1 - (.95 * .95 * .95), which is about 14 272 percent.  For ten endpoints, the Type I error rate is about 40 percent. 


# Example data
As an illustrative example, we consider published data from the phase-1 trial [NCT01922336](https://clinicaltrials.gov/study/NCT01922336#study-overview). In this trial, the following outcomes were observed after a single dose of SB2 or its EU-sourced reference product [@shin_randomized_2015]:

```{r setup, echo=FALSE}
data <- data.frame("PK measure" = c("AUCinf ($\\mu$g*h/mL)","AUClast ($\\mu$g*h/mL)","Cmax ($\\mu$g/mL)"),
                   "SB2" = c("38,703 $\\pm$ 11,114", "36,862 $\\pm$ 9133", "127.0 $\\pm$ 16.9"), 
                   "EU-INF" = c("39,360  $\\pm$ 12,332", "37,022 $\\pm$ 9398", "126.2 $\\pm$ 17.9"))

kableExtra::kable_styling(kableExtra::kable(data, 
                                            col.names = c("PK measure", "SB2", "EU-INF"),
                                            caption = "Primary PK measures between test and reference product. Data represent arithmetic mean +- standard deviation."),
                          bootstrap_options = "striped")

```

To provide the necessary information for the calculations, the user can input the mean (`mu_list`) and standard deviation information for each endpoint on each arm using lists of standard deviations (`sigma_list`). Since in this example, we only have the standard deviation of the endpoints, the user needs to provide correlation information between endpoints of each arm. This can be achieved through a correlation matrix (`cor_mat`) or by specifying a constant correlation parameter between endpoints (`rho`), which will be internally used to calculate a variance-covariance matrix across endpoints on each arm, if none of them are provided by default it is assumed no correlation between endpoints on all the arms (`rho=0`). However, if the user possesses the variance-covariance matrix per arm, they can include it as a list of matrices in the `varcov_list` parameter.

The function will automatically validate the information provided for each arm and exclude endpoints with missing values (NA) for either the mean or variance. Additionally, it is recommended that the user supplies a list of endpoint names (`ynames_list`) to help the program correctly assign the mean and variance information for each endpoint in each arm. As we only consider three endpoints in this example, we will specify `ynames_list` as the list of `AUCinf`, `AUClast`, and `Cmax`. 

The user must also provide information about the comparators using two lists: `list_comparators` and `list_y_comparator`. The `list_comparators` identifies the arms to be compared within each comparator, while the `list_y_comparator` specifies the endpoints to be compared within each comparator. If no `list_comparators` are provided, the function by default will use all possible pairwise comparisons across arms. Similarly, when `list_y_comparator` is not specified, it will compare all the endpoints common in both arms.

The function will verify if there is enough information available to perform the comparison, considering both mean and variance for each endpoint in each arm. If complete information is not provided, the function will perform comparisons only for endpoints with full information on each arm. In this case, as we only compare two arms, we will only compare SB2 and EU Remicade.


## No Correlation between Endpoints

In this setting, we assume a common standard deviation for each biosimilar, Ratio of Means (ROM) as parameter tested, parallel design, lognormal distribution assumption, no multiplicity adjustment, and equivalence is only needed for one of the endpoints. Besides, we assume that all endpoints are uncorrelated. This is specified with the default value of the rho parameter, which is set to $\rho = 0$.

```{r}
mu_list <- list(SB2 = c(AUCinf = 38703, AUClast = 36862, Cmax = 127.0),
                EUREF = c(AUCinf = 39360, AUClast = 37022, Cmax = 126.2))

sigma_list <- list(SB2 = c(AUCinf = 11114, AUClast = 9133, Cmax = 16.9),
                   EUREF = c(AUCinf = 12332, AUClast = 9398, Cmax = 17.9))
```

As we are comparing multiple endpoints, it's necessary to specify the lower and upper equivalence boundaries for each endpoint. For simplicity, we assume the same equivalent boundaries for all endpoints. Additionally, adjustments on the alpha confidence levels are required for multiple endpoint comparisons. More information about this will be provided in the alpha adjustment section. For now, we will not make any adjustments, which is the default option `adjust = "no"`. 
```{r}
# Equivalent boundaries
lequi.tol <- c(AUCinf = 0.8, AUClast = 0.8, Cmax = 0.8)
uequi.tol <- c(AUCinf = 1.25, AUClast = 1.25, Cmax = 1.25)
```

Also, we can specify the number of tests across endpoints that are required to be passed for equivalence. In this case, we use the default option where all specified co-primary endpoints have to be equivalent, denoted as $k = m = 3$.

Next, we proceed to run the function to calculate the sample size using the `calopt` function. We may also need to specify the function arguments to comply with the settings, such as `vareq`, `dtype`, `ctype`, `rho`, `adjust`, `k`, and `rho`. For brevity, we set these parameters to the default values of the function.

```{r}
library(simsamplesize)

output_mult <-  calopt(power = 0.9, # target power
                       alpha = 0.05,
                       mu_list = mu_list,
                       sigma_list = sigma_list,
                       lequi.tol = lequi.tol,
                       uequi.tol = uequi.tol,
                       ncores = 1,
                       nsim = 50,
                       seed = 1234)

print(output_mult$response)
```

The table above shows that for this setting, the required sample size for `r round(output_mult$response$power*100, 2)`% achieved power is `r output_mult$response$n_total`.

Using `plot.simms()` function, we can make the plot of sample size vs power achieved, as follows:

```{r }
plot.simss(output_mult)
```


## Correlation Exists between Endpoints

We proceed with the values that we used before, but on this cas we assume that there is a correlation between endpoints. Hence, we set the  $\rho = 0.6$. This means that the correlation between endpoints is the same across all endpoints. If the correlation is different between enndpoints, we can set it in the correlation matrix (`cor_mat`) instead. 

```{r}
# here, all of the setting is the same, except for rho = 0.6
output_mult_corr <-  calopt(power = 0.9, # target power
                            alpha = 0.05,
                            mu_list = mu_list,
                            sigma_list = sigma_list,
                            lequi.tol = lequi.tol,
                            uequi.tol = uequi.tol,
                            rho = 0.6,
                            ncores = 1,
                            nsim = 50)

print(output_mult_corr$response)

```

Referring to the output above, the required sample size for this setting is `r output_mult_corr$response$n_total`, which requires `r output_mult$response$n_SB2 - output_mult_corr$response$n_SB2` patient less than the scenario where endpoints are uncorrelated.

## Multiple comparison adjustments

In a hypothesis test, there is always a Type I error rate that tells us the probability of rejecting a null hypothesis that is actually true (false positive). When we perform one hypothesis test, the Type I error rate is equal to the significance level (α), commonly chosen to be 5%. However, when we conduct multiple hypothesis tests at once, such as multiple endpoint comparisons, the probability of getting a false positive increases.

For instance, in this case m=3 endpoints and an alpha level of 5%, the probability of getting a Type I error on at least one of the hypothesis tests is calculated as follows:

$$ 1 – (1-\alpha)^m  = 1 - (1-0.05)^3 = 0.1426. $$
So, the alpha level increases to 14%, being necessary to adjust it for multiple endpoint comparisons. Various methods have been proposed, particularly in the package we consider the followed:

### Bonferroni correction (bon):
Where the alpha is adjusted with the number of compared endpoints:
$$\alpha_{bon}= \alpha/m$$
This adjusment is specified through the adjust parameter as follows:

```{r}
output_mult_bon <-  calopt(power = 0.9, # target power
                           alpha = 0.05,
                           mu_list = mu_list,
                           sigma_list = sigma_list,
                           lequi.tol = lequi.tol,
                           uequi.tol = uequi.tol,
                           adjust = "bon",
                           ncores = 1,
                           nsim = 50)
print(output_mult_bon$response)

```
Based on the table above, the required sample is `r output_mult_bon$response$n_total`, which requires `r output_mult_bon$response$n_SB2-output_mult$response$n_SB2` additional patients per arm.


### Sidak correction (sid):

The Sidak correction assumes that tests are independent, so the probability that all the tests are not significant is the product of the probability that each of them is not significant, as follows:

$$\alpha_{sid}= 1-(1-\alpha)^ {1/m}$$
When we applied the Sidka adjustment in our previous example
```{r}
output_mult_sid <-  calopt(power = 0.9, # target power
                           alpha = 0.05,
                           mu_list = mu_list,
                           sigma_list = sigma_list,
                           lequi.tol = lequi.tol,
                           uequi.tol = uequi.tol,
                           adjust = "sid",
                           ncores = 1,
                           nsim = 50)
print(output_mult_sid$response)
```
Here the required sample is instead `r output_mult_sid$response$n_total`


### K adjustment (k):

It is a correction that is applied when the number of tests required for equivalence (k) is lower than the total number of equivalent tests per arm (m). The adjustment is given by:

$$\alpha_k= \frac{k*\alpha}{m}$$

This setting basically the same with the previous example. The difference lies in the adjustment which is *k* adjustment. Hence, all we need to do is change `adjust` argument to be `adjust = "k"`. If we apply this correction with `k = 3`, we would obtain the same sample size calculation as when no correction is applied. Therefore, here we consider an equivalent test when it is only required that 2 of the 3 endpoints are equivalent, as specified with the `k` parameter.

```{r}
output_mult_k <-  calopt(power = 0.9, # target power
                         alpha = 0.05,
                         mu_list = mu_list,
                         sigma_list = sigma_list,
                         lequi.tol = lequi.tol,
                         uequi.tol = uequi.tol,
                         adjust = "k",
                         k = 2,
                         ncores = 1,
                         nsim = 50)
print(output_mult_k$response)
```
With this setting, as we require less endpoints to be equivalent we get a fewer sample size `r output_mult_k$response$n_SB2` patients per arm, compared to the one using Bonferroni adjustment.

### Sequential adjustment (seq):

Here, it is required that the user specify which of the endpoints are primary and secondary in the `type_y` vector parameter. Then, tests are sequentially performed, considering first the test results on the primary endpoints, and then the tests on the secondary endpoints if the first group is accepted.

The alpha is separately adjusted over the groups of endpoints (primary and secondary). A Bonferroni adjustment is used only considering the number of primary endpoints. If the test is accepted on the primary endpoint, it is sequentially tested on the secondary endpoints, which have also been Bonferroni adjusted with the number of secondary endpoints.

In this example, we need to change the function argument to `adjust = "seq"` and also specify the type of endpoint through the `type_y` parameter. Here, we consider that "AUCinf" is a primary endpoint, while  "AUClast" and "Cmax" are secondary endpoints.


```{r}
output_mult_seq <-  calopt(power = 0.9, # target power
                           alpha = 0.05,
                           mu_list = mu_list,
                           sigma_list = sigma_list,
                           lequi.tol = lequi.tol,
                           uequi.tol = uequi.tol,
                           adjust = "seq",
                           type_y = c(1,2,2),
                           ncores = 1,
                           nsim = 50)
print(output_mult_seq$response)
```

Using sequential adjustment, the required sample size is `r output_mult_seq$response$n_total`.

# References
